{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd49b627",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.optimize import curve_fit \n",
    "import math\n",
    "import bisect\n",
    "\n",
    "# imports of our code \n",
    "from load_models import *\n",
    "import LocalLearning_copy as LocalLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11e602df",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(7)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37a4f318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters:\n",
    "BATCH_SIZE = 1000\n",
    "NUMBER_OF_EPOCHS = 1000\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "# loss function\n",
    "ce_loss = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4245e10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar10Train= LocalLearning.LpUnitCIFAR10(\n",
    "            root=\"../data/CIFAR10\",\n",
    "            train=True,\n",
    "            transform=ToTensor(),\n",
    "            p=3.0,\n",
    "        )\n",
    "\n",
    "cifar10Test= LocalLearning.LpUnitCIFAR10(\n",
    "            root=\"../data/CIFAR10\",\n",
    "            train=False,\n",
    "            transform=ToTensor(),\n",
    "            p=3.0,\n",
    "        )\n",
    "\n",
    "TestLoader = LocalLearning.DeviceDataLoader(\n",
    "            cifar10Test,\n",
    "            device=device,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            num_workers=4,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "TrainLoader = LocalLearning.DeviceDataLoader(\n",
    "            cifar10Train,\n",
    "            device=device,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            num_workers=4,\n",
    "            shuffle=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcc8496b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = [\"airplane\", \"car\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8ba2e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_critirium(\n",
    "    dataloader,\n",
    "    model, \n",
    "    crit,\n",
    "    thres = None\n",
    "    ):\n",
    "    \n",
    "    # Returns the data and the corresponding labels that meets the critirium given (crit)\n",
    "    \n",
    "    freq_correct = 0\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    \n",
    "    data = torch.zeros((0,32,32,3)).to(device)\n",
    "    lab_data = torch.zeros((0)).to(device)\n",
    "    \n",
    "    for batch_no, (features, labels) in enumerate(dataloader):\n",
    "        preds = model(features)\n",
    "        pred = torch.argmax(preds, dim=-1)\n",
    "        \n",
    "        if crit == \"correct\":\n",
    "            filtr_idx = (torch.abs(pred - labels) == 0)\n",
    "            new_features = features[filtr_idx]\n",
    "            data = torch.cat((data, new_features),dim=0)\n",
    "            new_labels = labels[filtr_idx]\n",
    "            lab_data = torch.cat((lab_data, new_labels),dim=0)\n",
    "        \n",
    "        elif crit == \"correct_thres\":\n",
    "            softmax_correct = (preds[torch.arange(1000),pred])\n",
    "            thres_idx = (softmax_correct >= thres)\n",
    "            correct_idx = (torch.abs(pred - labels) == 0)\n",
    "            filtr_idx = thres_idx & correct_idx \n",
    "            new_features = features[filtr_idx]\n",
    "            data = torch.cat((data, new_features),dim=0)\n",
    "            new_labels = labels[filtr_idx]\n",
    "            lab_data = torch.cat((lab_data, new_labels),dim=0)\n",
    "            \n",
    "        elif crit == \"thres\":\n",
    "            softmax_correct = (preds[torch.arange(1000),pred])\n",
    "            filtr_idx = (softmax_correct >= thres)\n",
    "            new_features = features[filtr_idx]\n",
    "            data = torch.cat((data, new_features),dim=0)\n",
    "            new_labels = labels[filtr_idx]\n",
    "            lab_data = torch.cat((lab_data, new_labels),dim=0)\n",
    "            \n",
    "        else: \n",
    "            raise ValueError(\"Not a valid criterium\")\n",
    "    \n",
    "    return data, lab_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9adc5ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_steps = 20000 \n",
    "step_size = 0.0001 \n",
    "eps_start = 0\n",
    "\n",
    "eps_list = [eps_start + n * step_size for n in range(max_num_steps + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e55c274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crit_eps(criterium, model, attack, print_accuracy=False):\n",
    "    \n",
    "    features, labels = data_critirium(TestLoader, model, criterium)\n",
    "    labels = labels.type(torch.LongTensor).to(device)\n",
    "    features.requires_grad = True\n",
    "    perturbed_image = features \n",
    "    \n",
    "    siz = len(labels)\n",
    "\n",
    "    crit_eps_per_image = torch.ones(siz).to(device).fill_(math.nan)   \n",
    "    crit_dist_per_image = torch.ones(siz).to(device).fill_(math.nan)\n",
    "\n",
    "    freq_correct = 0\n",
    "    total = 0 \n",
    "\n",
    "    loss_fn = ce_loss \n",
    "    optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    correct = []\n",
    "    b_norm = 0.05\n",
    "    \n",
    "    noise = torch.randn(features.shape).to(device)\n",
    "    \n",
    "    images = []\n",
    "    accuracy_dict = {}\n",
    "    accuracy_dict_actual = {}\n",
    "    \n",
    "    preds = model(features)\n",
    "    loss = loss_fn(preds, labels)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    with tqdm(total=len(eps_list)) as pbar:\n",
    "        for i, eps in enumerate(eps_list):\n",
    "        \n",
    "            with torch.no_grad():\n",
    "\n",
    "                if attack == \"WN\":\n",
    "                    adv_image = features + eps*noise\n",
    "                    perturbed_image = torch.clamp(adv_image, min = 0, max = 1)\n",
    "\n",
    "                elif attack == \"PGD\":\n",
    "\n",
    "                    adv_image = perturbed_image + eps*features.grad.data.sign()\n",
    "                    clamp = torch.clamp(adv_image - features, min = -b_norm, max = b_norm)\n",
    "                    perturbed_image = torch.clamp(features + clamp, min = 0, max = 1)\n",
    "\n",
    "                elif attack == \"FGSM\":\n",
    "\n",
    "                    perturbed_image = features + eps*features.grad.data.sign()\n",
    "                    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "\n",
    "                preds_perturbed = torch.argmax(model(perturbed_image), dim=-1)\n",
    "                alike = (preds_perturbed == labels)\n",
    "                freq_correct = (torch.abs(preds_perturbed - labels) == 0).sum()\n",
    "                total = len(labels)\n",
    "                \n",
    "                accuracy = (freq_correct/total).item()\n",
    "                correct.append(accuracy)\n",
    "                \n",
    "                mask = (alike == False) & (crit_eps_per_image.isnan())\n",
    "\n",
    "                x = features.view(features.size(0),-1)\n",
    "                y = perturbed_image.view(perturbed_image.size(0),-1)\n",
    "\n",
    "                dist = torch.norm(x - y, dim=1).detach()\n",
    "\n",
    "                crit_dist_per_image[mask] = dist[mask]\n",
    "\n",
    "                perturbation = torch.abs(features - perturbed_image).detach()\n",
    "                avg_perturbation = torch.sum(perturbation.view(perturbation.size(0), -1), dim=1) / (perturbation.size(1) * perturbation.size(2) * perturbation.size(3))\n",
    "                crit_eps_per_image[mask] = avg_perturbation[mask]\n",
    "                \n",
    "                accuracy_dict_actual[eps] = accuracy*100\n",
    "                \n",
    "                pbar.update(1)\n",
    "                \n",
    "                if (i in [0,100,150,200,500]) and (attack == \"WN\"):\n",
    "                    info = [perturbed_image, labels, preds_perturbed, features, eps]\n",
    "                    images.append(info)\n",
    "                    \n",
    "                if (i in [0,10,20,30,100]) and (attack == \"PGD\" or attack == \"FGSM\"):\n",
    "                    info = [perturbed_image, labels, preds_perturbed, features, eps]\n",
    "                    images.append(info)\n",
    "                    \n",
    "    unique_crit_eps = torch.unique(crit_eps_per_image)\n",
    "    for unique_eps in unique_crit_eps:\n",
    "        if not math.isnan(unique_eps.item()):\n",
    "            not_misclassified = (torch.sum(crit_eps_per_image > unique_eps).item()*100)/siz\n",
    "            accuracy_dict[unique_eps.item()] = not_misclassified\n",
    "    \n",
    "    if print_accuracy == True:\n",
    "        print(f\"{correct[-1]*100:.2f}% is still correctly classified\")\n",
    "        print(f\"{100*(torch.sum(crit_eps_per_image.isnan()).item())/siz}% have been correctly classified at every step\")\n",
    "        \n",
    "    crit_eps_per_image = np.array(crit_eps_per_image.cpu())\n",
    "    crit_dist_per_image = np.array(crit_dist_per_image.cpu())\n",
    "    correct = np.array(correct)\n",
    "        \n",
    "    return crit_eps_per_image, crit_dist_per_image, correct, images, accuracy_dict, accuracy_dict_actual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4092fb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxplot(ll_crit_eps, bp_crit_eps, attack, typ):\n",
    "    \n",
    "    # Remove NaN values from the input data\n",
    "    ll_crit_eps = ll_crit_eps[~np.isnan(ll_crit_eps)]\n",
    "    bp_crit_eps = bp_crit_eps[~np.isnan(bp_crit_eps)]\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.boxplot((ll_crit_eps, bp_crit_eps), showfliers=False)\n",
    "    ax.set_xticklabels(['LL', 'BP'])\n",
    "    if typ == \"eps\":\n",
    "        ax.set_ylabel('Critical epsilon')\n",
    "    elif typ == \"dist\":\n",
    "        ax.set_ylabel('Critical distance')\n",
    "        \n",
    "    ax.set_title(f'Konstantin model {attack} attack')\n",
    "    \n",
    "    median_ll = np.nanmedian(ll_crit_eps)\n",
    "    median_bp = np.nanmedian(bp_crit_eps)\n",
    "    ax.text(0.95, 0.95, f\"Median LL: {median_ll:.5f}\\nMedian BP: {median_bp:.5f}\", \n",
    "            verticalalignment='top', horizontalalignment='right',\n",
    "            transform=ax.transAxes, fontsize=12)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee20f832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy(ll, bp):\n",
    "    plt.plot(ll*100,label=\"ll\")\n",
    "    plt.plot(bp*100,label=\"bp\")\n",
    "    plt.ylabel(\"Accuracy [%]\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ae56ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy_vs_epsilon(accuracy_dict, actual_accuray_dict, modeltype):\n",
    "    crit_epsilons = sorted(accuracy_dict.keys())\n",
    "    accuracies = [accuracy_dict[crit_eps] for crit_eps in crit_epsilons]\n",
    "    crit_epsilons = [0] + crit_epsilons\n",
    "    accuracies = [100] + accuracies\n",
    "    \n",
    "    last_eps = crit_epsilons[-1]\n",
    "    \n",
    "    epsilons = sorted(actual_accuray_dict.keys())\n",
    "    last_eps_idx = bisect.bisect_left(epsilons, last_eps)\n",
    "    epsilons = epsilons[:last_eps_idx+1]\n",
    "    accuracies_actual = [actual_accuray_dict[eps] for eps in epsilons]\n",
    "        \n",
    "    title=\"Accuracy vs Epsilon \" + modeltype\n",
    "    \n",
    "    a = 0\n",
    "    b = -1\n",
    "    plt.figure()\n",
    "    plt.plot(crit_epsilons[a:b], accuracies[a:b],'o', label = \"Mean pertubation\")\n",
    "    plt.plot(epsilons[a:b], accuracies_actual[a:b], \"go\", label = \"ε in loop\")\n",
    "    plt.xlabel('Critical Epsilon')\n",
    "    plt.ylabel('Accuracy [%]')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9cbfd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 42\n",
    "\n",
    "def MinMaxNorm(x: np.array) -> np.array:\n",
    "        mi = np.amin(x)\n",
    "        ma = np.amax(x)\n",
    "        return (x - mi) / (ma - mi)\n",
    "\n",
    "def show_image(features):\n",
    "\n",
    "    f_np = features.detach().cpu().numpy()\n",
    "    HM = np.zeros((32, 32, 3))\n",
    "    HM = np.transpose(f_np[idx], (0, 1, 2))\n",
    "    HM = MinMaxNorm(HM)\n",
    "    nc = np.max(np.absolute(HM))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(HM, vmin=-nc, vmax=+nc)\n",
    "    \n",
    "def plot_images(images):\n",
    "    \n",
    "    N = len(images)\n",
    "    \n",
    "    ilist = [i for i in range(2,N+2)]\n",
    "    plt.figure(figsize=(15,5))\n",
    "    \n",
    "    plt.subplot(1,N+1,1)\n",
    "    show_image(images[0][3])\n",
    "    plt.title(f\"Actual = {label_names[images[0][1][idx]]}\")\n",
    "    \n",
    "    plt.axvline(x=33, color='k', linestyle='--')\n",
    "        \n",
    "    for image,i in zip(images[1:],ilist):\n",
    "        \n",
    "        plt.subplot(1,N+1,i)\n",
    "        show_image(image[0])\n",
    "        \n",
    "        predicted_idx = int(image[2][idx])\n",
    "        \n",
    "        plt.title(f\"{label_names[predicted_idx]}, ε = {image[-1]}\")\n",
    "        \n",
    "    #plt.suptitle(\"Actual, predicted\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd7e745e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for downloading models in load_models.py \n",
    "bp_modelK = load_Konstantin_model(\"bp_KHModel_kh_layer_cifar10.pty\",\"bp\")\n",
    "ll_modelK = load_Konstantin_model(\"ll_KHModel_kh_layer_cifar10.pty\",\"ll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acb5bc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "pa = False # print accuracy\n",
    "si = True # show images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9e96da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006119728088378906,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 66,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 20001,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aaaa5019a8642d3b6ba59569be8677c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005896329879760742,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 66,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 20001,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f5b331e691646a9b5eb6e262e5bb751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "criteps_bp_wn, critdist_bp_wn, correct_bp_wn, images_bp_wn, accuracy_dict_bp_wn, accuracy_act_bp_wn = crit_eps(\"correct\", bp_modelK, \"WN\", print_accuracy=pa)\n",
    "criteps_ll_wn, critdist_ll_wn, correct_ll_wn, images_ll_wn, accuracy_dict_ll_wn, accuracy_act_ll_wn = crit_eps(\"correct\", ll_modelK, \"WN\", print_accuracy=pa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f5cbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy_vs_epsilon(accuracy_dict_bp_wn, accuracy_act_bp_wn, \"bp\")\n",
    "plot_accuracy_vs_epsilon(accuracy_dict_ll_wn, accuracy_act_ll_wn,  \"ll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e2083e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(images_bp_wn)\n",
    "plot_images(images_ll_wn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe47b3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot(criteps_ll_wn, criteps_bp_wn, \"WN\", \"eps\")\n",
    "boxplot(critdist_ll_wn, critdist_bp_wn, \"WN\", \"dist\")\n",
    "plot_accuracy(correct_ll_wn,correct_bp_wn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967cdb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "criteps_bp_pgd, critdist_bp_pgd, correct_bp_pgd, images_bp_pgd, accuracy_dict_bp_pgd, accuracy_act_bp_pgd = crit_eps(\"correct\", bp_modelK, \"PGD\", print_accuracy=pa)\n",
    "criteps_ll_pgd, critdist_ll_pgd, correct_ll_pgd, images_ll_pgd, accuracy_dict_ll_pgd, accuracy_act_ll_pgd = crit_eps(\"correct\", ll_modelK, \"PGD\", print_accuracy=pa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9417c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy_vs_epsilon(accuracy_dict_bp_pgd, accuracy_act_bp_pgd, \"bp\")\n",
    "plot_accuracy_vs_epsilon(accuracy_dict_ll_pgd, accuracy_act_ll_pgd, \"ll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38ced25",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(images_bp_pgd)\n",
    "plot_images(images_ll_pgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81470808",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot(criteps_ll_pgd, criteps_bp_pgd, \"PGD\", \"eps\")\n",
    "boxplot(critdist_ll_pgd, critdist_bp_pgd, \"PGD\", \"dist\")\n",
    "plot_accuracy(correct_ll_pgd[:50],correct_bp_pgd[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0561f0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "criteps_bp_fgsm, critdist_bp_fgsm, correct_bp_fgsm, images_bp_fgsm, accuracy_dict_bp_fgsm, accuracy_act_bp_fgsm = crit_eps(\"correct\", bp_modelK, \"FGSM\", print_accuracy=pa)\n",
    "criteps_ll_fgsm, critdist_ll_fgsm, correct_ll_fgsm, images_ll_fgsm, accuracy_dict_ll_fgsm, accuracy_act_ll_fgsm = crit_eps(\"correct\", ll_modelK, \"FGSM\", print_accuracy=pa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb66855c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy_vs_epsilon(accuracy_dict_bp_fgsm, accuracy_act_bp_fgsm, \"bp\")\n",
    "plot_accuracy_vs_epsilon(accuracy_dict_ll_fgsm, accuracy_act_ll_fgsm, \"ll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ce88d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(images_bp_fgsm)\n",
    "plot_images(images_ll_fgsm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8e01a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot(criteps_ll_fgsm, criteps_bp_fgsm, \"FGSM\", \"eps\")\n",
    "boxplot(critdist_ll_fgsm, critdist_bp_fgsm, \"FGSM\", \"dist\")\n",
    "plot_accuracy(correct_ll_fgsm[:50],correct_bp_fgsm[:50])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
